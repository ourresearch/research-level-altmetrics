---
title: 'Researcher-level altmetrics paper'
author: "Heather Piwowar and Jason Priem"
output:
  html_document:
    fig_caption: yes
    force_captions: yes
    highlight: pygments
    keep_md: yes
    number_sections: no
    pandoc_args:
    - +RTS
    - -K10000000
    - -RTS
  md_document:
    variant: markdown_github
  pdf_document:
    fig_caption: yes
    keep_md: yes
    keep_tex: yes
    latex_engine: xelatex
---

*We're presenting this paper at the [3:AM conference](http://altmetricsconference.com/schedule/) in Bucharest, 28-29 Sept 2016. Once it's a bit more polished we plan to upload it to the ArXiv. It's very much a work in progress; feel free to submit pull requests with updates and changes.*

```{r setup, echo=FALSE}
library(knitr)
opts_knit$set(progress=TRUE, tidy=TRUE, echo=TRUE)
```

```{r knitcitations, echo=FALSE}
  # devtools::install_github("cboettig/knitcitations@v1")
  library(knitcitations); cleanbib()
  cite_options(citation_format = "pandoc", check.entries=FALSE)
  library(bibtex)
```

```{r load_libraries, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
library(magrittr)
library(ggplot2)
library(dplyr)
library(broom)
library(RJDBC)
library(reshape)
library(plyr); library(dplyr)
library(scales)
library(corrplot)
```

```{r get_data, echo=FALSE, results='hide', cache=TRUE}

profiles <- read.csv("profiles.csv")
```

## researcher-level altmetrics

_This section still needs its citations..._
 
There's been a lot of work examining the distribution and properties of altmetrics at the level of individual papers, and sensibly so. Since this is the lowest level of aggregation, it's a great place to start. Researchers have also looked at altmetrics by journal, following a long trend of journal-level citation bibliometrics work. However, there's surprisingly little work on altmetrics at the *researcher* level. 

And the researcher level matters. Researchers decide what to study, where to publish, and how to publish it. And they are exceedingly aware of impact metrics, and the massive affect of these metrics on their careers. So, effective and meaningful researcher-level altmetrics could powerfully impact scholarly communication.

Of course, altmetrics will take some time to become effective and meaningful. But in the meantime, it's useful to examine the distribution and character of researcher-level altmetrics, to test the potential of these metrics for real evaluation. 


## ORCID as a source for researcher-level altmetrics 
Finding data at the reseacher level is not simple, since many commercial researcher databases like Google Scholar (shamefully) prohibit systematic analysis of their data. However, the growth of the ORCID ID system gives us an opportunity to study and open dataset of scholars and their publications. And the [2015 ORCID public data file](https://figshare.com/articles/ORCID_Public_Data_File_2015/1582705) is available for download and re-use.

Using ORCID profiles does have some difficulties. Unlike systems such as Google Scholar, ORCID does not automatically add a users' publications. It must be done manually, and it is not a particularly user-friendly process. Because of this, most ORCIDs do not include a complete publication list. This is particularly true since most  ORCIDs are created as part of a single-signon flow for another system (an academic version of the ubiquitous "sign in with Facebook" button). These users just want an account, and have no interest in making a complete publication list.

We solved this problem by using clues in ORCID records to find users who have spent significant amounts of time editing their profiles, on the assumption that these users are likely to have assembled a relatively complete publication list. After trying several different clues, we settled on two: users were more likely to have complete publications list if they 

1. Had at least one publication with a DOI from 2015 and
2. Had added a custom URL to their homepage.

This significantly narrowed our sample from the 2015 public data file, from 1.6M to 17k:

| Total ORCID accounts          | 1.6M |
|-------------------------------|------|
| With DOIs                     | 215k |
| With 2015 DOIs                | 62k  |
| With 2015 DOIs and custom URL | 17k  |

## How much altmetrics activity to these researchers have?

To start with, let's look at the number of times each researcher's work has been mentioned on a social platform (Twitter, Facebook, etc). We'll use Altmetric.com to do this, since they do a great job of getting all this data. 

One downside of this approach is that we're only able to track mentions (Altmetic calls them "posts") of articles with DOIs (well, technically ArXiv IDs as well, but this is a very small number). That still leaves us with plenty of publications to work with though, since most publications in ORCID profiles do have DOIs *(put numbers here later).*


| number of mentions | number of profiles in sample |
|-------------------------------|------|
| Fewer than 3   | 4763 |
| 4 -19   | 5390  |
| 20-200 | 5239  |
| More than 200 | 1344 |

There are 6583 ( 39% ) of the samples profiles have more than 20 mentions. For the rest of this analysis, we'll focus on these profiles, for three reasons:

1. They are a meaningful percentage of the dataset
2. The higher n for each profiles gives us more statistical ability to learn about altmetrics at the person level.
2. And they are a particularly interesting part of the dataset, because it gets us all the "early adopters" and then some, based on Rogers' [Diffusion of Innovations](https://en.wikipedia.org/wiki/Diffusion_of_innovations) framework *(cite)*. Moreover, our experience with Impactstory users has shown that users with more than 20 mentions [are much more likely to be recommend altmetrics tools](http://blog.impactstory.org/nps/) like Impactstory. 


## What kind of online impact are they having?

What platforms are researchers' work being mentioned on? This is important, because of course each one mention is not the same as another. In particular, we know that researchers have different awareness of different kinds of online platforms. We're calling these different platforms or sources "channels."

```{r}
# by metric
number_of_people = nrow(profiles)
channels = profiles %>% select(blogs:wikipedia) %>% melt
nonzero_channels =  channels %>% filter(value>1)

nonzero_channels %>% ggplot( aes(reorder(variable, variable, length)) ) +
  geom_bar(aes(y=..count../6583)) +
  coord_flip() +
  scale_y_continuous(labels = percent) +
  ggtitle("Percentage people with research mentioned on a given channel \n(people with at least 20 mentions)") +
  ylab("Percentage of people") +
  xlab("Channel")
```


So we can see that the majority of researchers in this set have mentions on Twitter, Facebook, news outlets, and blogs. We can look at the number of channels per researchers more explicitly by plotting how many distinct channels each profile has mentions from. We can see that the number of channels is centered around four, with 3104 people (47%) mentioned on 3, 4, or 5 channels:

```{r}
# number of sources
profiles %>% ggplot( aes(num_sources) ) + geom_histogram(binwidth=1)

```


## how much online impact are they having?

There are a few ways to look at the total amount of online impact each researcher is having. We'll examine three:

* counts of mentions
* source-weighted mention counts using Altmetric score
* Twitter h-index ("t-index")

### Counts of mentions
The distribution of mentions per person is, as expected, highly skewed. *(later: show log-log post because we think it is log-log distributeds)*. The median number of mentions per profile is 68.

```{r}

# number of posts
profiles$posts %>% median
```

### Source-weighted mention counts using Altmetric score

The total count is interesting, but of course mentions are not all created equal, and just summing mentions sweeps this under the rug. There's always been much interest in altmetrics research in weighting different types of mentions. There are a number of different kinds of weights we could use, depending on the questions we're asking;if we're interested in research impact we might look at F1000 mentions and online peer reviews, but if we're interested in impact on online discussion we might more heavily weight Facebook.

Be far the most used weighting scheme, in practice, is the Altmetric Score. The score is *mostly* open and [details are available online](https://help.altmetric.com/support/solutions/articles/6000060969-how-is-the-altmetric-attention-score-calculated-), although some of the finer-grained weightings are not available. We can use the weights shared to approximate an Altmetric Score at the person level, by multiplying each mention of a person's work by its weight, then summing these. (It's an approximation because some weights are not shared online, eg of specific news sources, and also because Twitter weights are based on number of unique *tweeters* not Tweets.)

Like the pure counts, the weighted count sums are highly skewed. However, they have an interesting property that is not true of pure mention counts: when we log-transform the bins, we get a relatively normal-shaped distribution, which is quite useful in many contexts. This difference is largely true because of the lower weight given to tweets in this system, which causes them to skew the distribution less.
```{r}

# altmetric scores
profiles %>% ggplot( aes(altmetric_score) ) + geom_histogram() + scale_x_log10() 
```

#### Relationship between weighted and unweighted counts

We wondered how much additional value is being added with the weighted approach. Are we getting at truly different picture of impact this way? To look more closely at this we plotted the weighted vs unweighted mention counts:

```{r}

# altmetric score vs posts
profiles %>%  ggplot(aes(posts, altmetric_score))  + geom_point(alpha=.2) + scale_x_log10() + scale_y_log10()

# using nonparametric cor because massive skew
cor(profiles$altmetric_score, profiles$posts, use='complete.obs', method="spearman")

```

Interestingly, there is quite a bit of correlation here, suggesting that (at a high level of aggregation) maybe a simple count of mentions is giving us a lot of the information we'd get from a weighted sum. We pushed this a bit further to see if there's good correlation between simply counting the summed number of tweets per profile, and using the summed Altmetric score:

```{r}



# altmetric score vs twitter
profiles %>%  ggplot(aes(twitter, altmetric_score))  + geom_point(alpha=0.2) + scale_x_log10() + scale_y_log10()
cor(profiles$altmetric_score, profiles$twitter, use='complete.obs', method="spearman")


```

Again, we see that there is a strong relationship. This is both because tweets make up a large part of the Altmetric score (since they are so prevelant), and because impact on different social channels tends to be fairly well-correlated as much previous research has shown *(cite)*. It's interesting to thing that at a coarse level, we can estimate a lot about the altmetrics impact of researchers by simply counting the number of times their research has been tweeted. 

Of course we would not want to do evaluate individuals using this relationship since there are numerous outliers. Future research should look more closely into these outliers, investigating what kind of people tend to have an unsually high or low percentage of "low-value" or "high-value" channels. And more importantly, we need more research on ways to weight mentions that go beyond simply noting the channel--since the "value" of a tweets, for instance, depends greatly on its context and author.


### Twitter h-index ("t-index")

A downside of these person-level stats is that they donâ€™t account for differing amounts of publications, or consistency across publications. Let's consider a t-index (Twitter-menion based h-index) which, reflects the number of publications as well as the number of mentions per publication. The definition of the t-index is that a scholar with an index of t has t publications each of which has been mentioned on twitter at least t times.

Half of our sample has a t-index of 3 or more:

```{r}

# altmetric score vs t-index
profiles %>% ggplot( aes(t_index) ) + geom_histogram(binwidth=1, aes(y=cumsum(..count..)))
profiles$t_index %>% median
```


We find the t-index has moderate to high correlation with both the Altmetrics score and the raw posts count. This is unsurprising given that the traditional h-index has also been shown to [correlate well with raw citation counts](http://eprints.rclis.org/8790/).


```{r}

profiles %>%  ggplot(aes(t_index, altmetric_score))  + geom_jitter(alpha=0.2) + scale_x_log10() + scale_y_log10()
cor(profiles$altmetric_score, profiles$t_index, use='complete.obs', method="spearman")
cor(profiles$posts, profiles$t_index, use='complete.obs', method="spearman")

```

## Still in progress

As you can see this is still very much a work in progress. Here's what we're still working on:

* For most people, most mentions are tweets. It will be cool to have a plot that shows this.
* We have data for the distribution of mentions over time. We need to describe this still.
* It's going to be really interesting to describe and plot the relationship between altmetric-based indicators (number of mentions, weighted mention counts, t-index) and traditional citation metrics for our sample. This has been done to death at the publication level, but not yet at person level so much.




